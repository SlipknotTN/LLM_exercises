{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "The dataset is the parsed version of the Wikipedia page https://it.wikipedia.org/wiki/Castelnuovo_di_Garfagnana in Italian language.\n",
    "\n",
    "It is an interesting use case because the trained model is able to answer something, but it mades a lot of errors. The data is probably only partially present in the training dataset or under-represented.\n",
    "\n",
    "Using RAG it is possible to see how the answers correctness improve, although they tend to be more coincise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Parse the Wikipedia page and create a pandas DataFrame with a \"text\" column with a sentence in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee8ed44-1596-4ae1-ae3f-b069e46534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_TITLE = \"Castelnuovo di Garfagnana\"\n",
    "OUTPUT_DATA_FILEPATH = \"./data/wiki_it_castelnuovo_garfagnana_nb.csv\"\n",
    "WIKIPEDIA_LANG = \"it\"\n",
    "SKIP_SECTIONS = [\"Collegamenti_esterni\", \"Altri_progetti\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: probably there is a nested list; it will be squashed into a single level, list element content: 'Liceo Scientifico Galileo Galilei; I.P.S.I.A. Simone Simoni (indirizzi: elettrici-meccanici); I.T.I. (indirizzi: elettrotecnica-meccanica e meccatronica)'\n",
      "WARNING: probably there is a nested list; it will be squashed into a single level, list element content: 'I.T.C.G. Luigi Campedelli (indirizzi: geometri-ragioneria)'\n",
      "80 sentences obtained from the page 'Castelnuovo di Garfagnana'\n",
      "CSV file saved to './data/wiki_it_castelnuovo_garfagnana_nb.csv'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_dict_key_from_headings(\n",
    "    last_h2_level_paragraph: str,\n",
    "    last_h3_level_paragraph: Optional[str] = None,\n",
    "    last_h4_level_paragraph: Optional[str] = None,\n",
    ") -> str:\n",
    "    key = f\"{last_h2_level_paragraph}\"\n",
    "    if last_h3_level_paragraph is not None:\n",
    "        key += f\" - {last_h3_level_paragraph}\"\n",
    "    if last_h4_level_paragraph is not None:\n",
    "        key += f\" - {last_h4_level_paragraph}\"\n",
    "    return key\n",
    "\n",
    "\n",
    "def get_cleaned_text(element) -> str:\n",
    "    \"\"\"\n",
    "    Strip text and remove '\\n' inside the paragraph\n",
    "    \"\"\"\n",
    "    return element.get_text().strip().replace(u\"\\xa0\",\" \").replace(\"\\n\", \" \")\n",
    "\n",
    "# \"query\" action documentation: https://en.wikipedia.org/w/api.php?action=help&modules=query\n",
    "# Don't pass \"explaintext\": 1 to get the text in HTML format. It is a bit more complex to parse, but we have\n",
    "# all the information to understand when a list is present\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"exlimit\": 1,\n",
    "    \"titles\": PAGE_TITLE,\n",
    "    \"exsectionformat\": \"wiki\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "resp = requests.get(\n",
    "    f\"https://{WIKIPEDIA_LANG}.wikipedia.org/w/api.php\", params=params\n",
    ")\n",
    "response_dict = resp.json()\n",
    "\n",
    "page_dict = next(iter(response_dict[\"query\"][\"pages\"].values()))\n",
    "title = page_dict[\"title\"]\n",
    "html_text = page_dict[\"extract\"]\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "# Enable for DEBUG\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Use BeatifulSoap the go element by element\n",
    "# headings -> new sectopm level\n",
    "# <p>...</p> sentences\n",
    "# <ul><li>..</li><li>...</li>...</ul> <li> elements to merge\n",
    "sentences_dict = defaultdict(list)\n",
    "last_h2_level_paragraph = None\n",
    "last_h3_level_paragraph = None\n",
    "last_h4_level_paragraph = None\n",
    "for element in soup:\n",
    "    if type(element) == bs4.Tag:\n",
    "        if element.name == \"p\" and last_h2_level_paragraph is None:\n",
    "            # Intro before the first headings\n",
    "            sentences_dict[title].append(get_cleaned_text(element))\n",
    "        elif element.name == \"h2\":\n",
    "            # First level paragraph\n",
    "            last_h2_level_paragraph = element.attrs[\"data-mw-anchor\"]\n",
    "            last_h3_level_paragraph = None\n",
    "            last_h4_level_paragraph = None\n",
    "            building_list = False\n",
    "        elif element.name == \"h3\":\n",
    "            # Second level paragraph\n",
    "            last_h3_level_paragraph = element.attrs[\"data-mw-anchor\"]\n",
    "            last_h4_level_paragraph = None\n",
    "            building_list = False\n",
    "        elif element.name == \"h4\":\n",
    "            # Third level paragraph\n",
    "            last_h4_level_paragraph = element.attrs[\"data-mw-anchor\"]\n",
    "            building_list = False\n",
    "        elif element.name == \"p\":\n",
    "            # Sentence of a paragraph\n",
    "            # Concatenate the headings to provide context\n",
    "            key = get_dict_key_from_headings(\n",
    "                last_h2_level_paragraph,\n",
    "                last_h3_level_paragraph,\n",
    "                last_h4_level_paragraph,\n",
    "            )\n",
    "            # Search for <ul> inside <p>\n",
    "            for p_children in element.children:\n",
    "                if type(p_children) == bs4.Tag and p_children.name == \"ul\":\n",
    "                    raise ValueError(\"List <ul> inside a <p> not supported\")\n",
    "            sentences_dict[key].append(get_cleaned_text(element))\n",
    "\n",
    "        elif element.name == \"ul\" or element.name == \"dl\":\n",
    "            # Get the list elements and merge them when necessary\n",
    "            # DO NOT MERGE when there is no sentence before ending with \":\"\n",
    "            # MERGE when the previous sentence ends with \":\" or when another list element is preceding\n",
    "\n",
    "            # Logic to merge the list elements\n",
    "            list_content_str = \"\"\n",
    "            for list_element in element.children:\n",
    "                if (\n",
    "                    list_element.name == \"li\"\n",
    "                    or list_element.name == \"dd\"\n",
    "                    or list_element.name == \"dt\"\n",
    "                ):\n",
    "                    list_text = get_cleaned_text(list_element)\n",
    "                    list_content_str += list_text + \"\\n\"\n",
    "            list_content_str = (\n",
    "                list_content_str.replace(\"\\n\", \"; \")\n",
    "                .replace(\",;\", \";\")\n",
    "                .replace(\";;\", \";\")\n",
    "                .replace(\".;\", \";\")[: -len(\", \")]\n",
    "            )\n",
    "            key = get_dict_key_from_headings(\n",
    "                last_h2_level_paragraph,\n",
    "                last_h3_level_paragraph,\n",
    "                last_h4_level_paragraph,\n",
    "            )\n",
    "            last_sentence_for_key = (\n",
    "                sentences_dict[key][-1] if len(sentences_dict[key]) > 0 else \"\"\n",
    "            )\n",
    "            if last_sentence_for_key.endswith(\":\"):\n",
    "                # Concatenate the list elements with the previous sentence which explains the list content,\n",
    "                sentences_dict[key][-1] += \" \" + list_content_str\n",
    "            elif last_element_type == \"ul\" or last_element_type == \"dl\":\n",
    "                # The list could already been started with a different ul or dl element,\n",
    "                # in this case we don't support nesting and we simply concatenate\n",
    "                print(\n",
    "                    f\"WARNING: probably there is a nested list; it will be squashed into a single level, list element content: '{list_content_str}'\"\n",
    "                )\n",
    "                sentences_dict[key][-1] += \"; \" + list_content_str\n",
    "            else:\n",
    "                # The list is probably part of an entire section and not introduce with \":\",\n",
    "                # so it does worth keeping split\n",
    "                sentences_dict[key].extend(list_content_str.split(\"; \"))\n",
    "        else:\n",
    "            raise ValueError(f\"Tag {element.name} not supported\")\n",
    "        last_element_type = element.name\n",
    "\n",
    "df_content = {\"text\": []}\n",
    "skip_keys_start = tuple(\n",
    "    [skip_section + \" - \" for skip_section in SKIP_SECTIONS]\n",
    ")\n",
    "for key, key_sentences in sentences_dict.items():\n",
    "    if key not in SKIP_SECTIONS and key.startswith(skip_keys_start) is False:\n",
    "        for key_sentence in key_sentences:\n",
    "            if key_sentence != \"\":\n",
    "                df_content[\"text\"].append(f\"{key} - {key_sentence}\")\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_content)\n",
    "print(f\"{len(df)} sentences obtained from the page '{PAGE_TITLE}'\")\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_DATA_FILEPATH), exist_ok=True)\n",
    "df.to_csv(OUTPUT_DATA_FILEPATH)\n",
    "print(f\"CSV file saved to '{OUTPUT_DATA_FILEPATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a595980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castelnuovo di Garfagnana - Castelnuovo di Gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geografia_fisica - Territorio - Sorge alla con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geografia_fisica - Clima - Classificazione sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geografia_fisica - Clima - Classificazione cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geografia_fisica - Clima - Diffusività atmosfe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Castelnuovo di Garfagnana - Castelnuovo di Gar...\n",
       "1  Geografia_fisica - Territorio - Sorge alla con...\n",
       "2  Geografia_fisica - Clima - Classificazione sis...\n",
       "3  Geografia_fisica - Clima - Classificazione cli...\n",
       "4  Geografia_fisica - Clima - Diffusività atmosfe..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3fb7742-b31f-45b5-82e0-ce93b93ce606",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAREUM_OPENAI_API_KEY = \"YOUR API KEY\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "REQUEST_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582f0656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n",
      "'embeddings' column added to the dataframe\n",
      "Embeddings space size using text-embedding-ada-002: 1536\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = VOCAREUM_OPENAI_API_KEY\n",
    "\n",
    "# Get the embeddings\n",
    "# In order to avoid a `RateLimitError` the data is sent in batches to the `Embedding.create` function\n",
    "print(\"Extracting embeddings...\")\n",
    "embeddings = []\n",
    "for i in range(0, len(df), REQUEST_SIZE):\n",
    "    # Send text data to OpenAI model to get embeddings, the embeddings are at sentence level, not word\n",
    "    response = openai.Embedding.create(\n",
    "        input=df.iloc[i : i + REQUEST_SIZE][\"text\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME,\n",
    "    )\n",
    "\n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "# Add embeddings list formatted as numpy array to the dataframe\n",
    "df[\"embeddings\"] = embeddings\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(np.array)\n",
    "\n",
    "print(\"'embeddings' column added to the dataframe\")\n",
    "print(\n",
    "    f\"Embeddings space size using {EMBEDDING_MODEL_NAME}: {len(embeddings[0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castelnuovo di Garfagnana - Castelnuovo di Gar...</td>\n",
       "      <td>[0.017287807539105415, -0.0054532866925001144,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geografia_fisica - Territorio - Sorge alla con...</td>\n",
       "      <td>[0.015181456692516804, -0.007471294142305851, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geografia_fisica - Clima - Classificazione sis...</td>\n",
       "      <td>[0.019138965755701065, -0.0005533070070669055,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geografia_fisica - Clima - Classificazione cli...</td>\n",
       "      <td>[0.022875944152474403, -0.00048568969941698015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geografia_fisica - Clima - Diffusività atmosfe...</td>\n",
       "      <td>[-0.0010899270419031382, 0.003765960456803441,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Castelnuovo di Garfagnana - Castelnuovo di Gar...   \n",
       "1  Geografia_fisica - Territorio - Sorge alla con...   \n",
       "2  Geografia_fisica - Clima - Classificazione sis...   \n",
       "3  Geografia_fisica - Clima - Classificazione cli...   \n",
       "4  Geografia_fisica - Clima - Diffusività atmosfe...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.017287807539105415, -0.0054532866925001144,...  \n",
       "1  [0.015181456692516804, -0.007471294142305851, ...  \n",
       "2  [0.019138965755701065, -0.0005533070070669055,...  \n",
       "3  [0.022875944152474403, -0.00048568969941698015...  \n",
       "4  [-0.0010899270419031382, 0.003765960456803441,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0eb3f-cfc9-4b65-afbd-2539b4cbe8ad",
   "metadata": {},
   "source": [
    "### Utility functions to exploit the ambeddings to answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_sorted_by_relevance(\n",
    "    question: str, df: pd.DataFrame, embedding_model_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that takes in input a question string, a dataframe and an embedding model name.\n",
    "    Each dataframe row includes a text and the associated embeddings vector.\n",
    "\n",
    "    Returns:\n",
    "        Copy of the input dataframe sorted by descending question relevance\n",
    "    \"\"\"\n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=embedding_model_name)\n",
    "\n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings, df_copy[\"embeddings\"].values, distance_metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def count_tokens(text: str, encoding: str = \"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens before calculating the embeddings\n",
    "\n",
    "    Args:\n",
    "        text: text for which you want to count the tokens\n",
    "        encoding: encoding name\n",
    "\n",
    "    Returns:\n",
    "        the number of tokens to represent the text\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(encoding)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def answer_question(question: str) -> List[str]:\n",
    "    # Create the embeddings for the question using under the hood openai.Embedding.create\n",
    "    df_sorted_distances = get_rows_sorted_by_relevance(\n",
    "        question=args.question,\n",
    "        df=df_embed,\n",
    "        embedding_model_name=args.embedding_model_name,\n",
    "    )\n",
    "\n",
    "    if args.closest_sentences_output_filepath:\n",
    "        df_sorted_distances.to_csv(args.closest_sentences_output_filepath)\n",
    "\n",
    "    # Create the prompt with a template to get an answer to the question\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question based on the context below, and if the question\n",
    "    can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "    Context: \n",
    "\n",
    "    {}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Question: {}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # We want to exploit the available number of tokens for the model, but with a limit, because we are charged based\n",
    "    # on the number of tokens\n",
    "    current_token_count = count_tokens(prompt_template) + count_tokens(args.question)\n",
    "    print(f\"Prompt template + question number of tokens: {current_token_count}\")\n",
    "\n",
    "    # Add context until max tokens (which can be exceeded with the last step)\n",
    "    context = []\n",
    "    for text in df_sorted_distances[\"text\"].values:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = count_tokens(text)\n",
    "        current_token_count += text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max.\n",
    "        # The last step can exceed max_prompt_tokens\n",
    "        if current_token_count <= args.max_prompt_tokens:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create the prompt with the context in a specific format to highlight each line (event)\n",
    "    prompt = prompt_template.format(\"\\n\\n###\\n\\n\".join(context), args.question)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Prompt tokens: {count_tokens(prompt)}\")\n",
    "\n",
    "    # From the documentation: the token count of your prompt plus max_tokens cannot exceed the model's context length.\n",
    "\n",
    "    # Answer without using the context\n",
    "    initial_answer = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=args.question,\n",
    "        max_tokens=args.max_answer_tokens,\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    print(f\"Initial answer: {initial_answer}\")\n",
    "\n",
    "    # Answer using the context\n",
    "    answer_with_context = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\", prompt=prompt, max_tokens=args.max_answer_tokens\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    print(f\"RAG answer: {answer_with_context}\")\n",
    "\n",
    "    # TODO: return the two answers before and after RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901c850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
